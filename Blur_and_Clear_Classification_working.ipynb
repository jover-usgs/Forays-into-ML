{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Blur and Clear NN Notebook\n",
    "https://github.com/aditya9211/Blur-and-Clear-Classification\n",
    "\n",
    "### General Quick Start Steps\n",
    "1. Clone repo https://github.com/aditya9211/Blur-and-Clear-Classification.git\n",
    "2. install dependencies- recommend creating a conda env first\n",
    "    a. create a conda enivironment with a yml file `conda env create -f BlurAndClear.yml` \n",
    "    b. `pip install -r requirements.txt`\n",
    "3. Train the network -  train.py with folders with good and bad example data\n",
    "4. Test the network - test.py\n",
    "5. Predict Output - predict.py --img imagename.jpg   THIS NEEDS WORK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F:\\CACO_HOM\\Blur-and-Clear-Classification\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.chdir('F:/CACO_HOM/Blur-and-Clear-Classification/') #Change to wherever you cloned the repo to\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install dependencies using one of the methods in quick start step 2 then proceed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The yml file should have the following organization\n",
    "\"\"\"\n",
    "name: BlurAndClear\n",
    "channels:\n",
    "    - conda-forge\n",
    "    - defualts\n",
    "dependencies:\n",
    "    - python = 3.5\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import argparse\n",
    "from sklearn.externals import joblib\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import scipy.misc as ms\n",
    "import scipy.ndimage as nd\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "import shutil\n",
    "#These are helper functions in train/test and predict - located in directory\n",
    "from utils import path_validation, resize, sigmoid, h, validate, model_score     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are all of the hyper-parameters and lists of constants in the config.py script. These can be edited here or uncomment the box below `from config import *`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F:\\CACO_HOM\\Blur-and-Clear-Classification\n"
     ]
    }
   ],
   "source": [
    "# Stores current executing path\n",
    "HOME_FOLDER_PATH = os.getcwd()\n",
    "print(os.getcwd())\n",
    "\n",
    "# if not passes default model path\n",
    "MODEL_PATH = HOME_FOLDER_PATH + '/model/result.pkl'\n",
    "DATA_PATH = HOME_FOLDER_PATH + '/data'\n",
    "\n",
    "# Path where splitted data get stored\n",
    "TRAIN_DATA_PATH = DATA_PATH + '/train_images.npy'\n",
    "TRAIN_LABEL_PATH = DATA_PATH + '/train_labels.npy'\n",
    "TEST_DATA_PATH = DATA_PATH + '/test_images.npy'\n",
    "TEST_LABEL_PATH = DATA_PATH + '/test_labels.npy'\n",
    "\n",
    "# Path for saving plot of cost vs iterations\n",
    "PLOT_PATH = DATA_PATH + '/loss_decay.png'\n",
    "\n",
    "\n",
    "# Median Filter size\n",
    "radius = 3\n",
    "\n",
    "# width and height of resized image\n",
    "width = 100\n",
    "height = 100\n",
    "\n",
    "# Splitting ratio for training & testing\n",
    "split_ratio = 0.4\n",
    "\n",
    "# Seed to get same results on re-run\n",
    "seed = 10\n",
    "\n",
    "# Size of data to be fed at each epochs\n",
    "batch_size = 10\n",
    "\n",
    "# Hidden Layer neurons size\n",
    "Neurons_size = 300\n",
    "\n",
    "# Iteration in NN Model\n",
    "max_iter = 50\n",
    "\n",
    "# Logging steps to show summary\n",
    "logging_steps = min(1, max_iter)\n",
    "\n",
    "# Learning rate in NN Model\n",
    "alpha = 0.001\n",
    "\n",
    "# Regularization Term used in cost\n",
    "Lambda = 0.0007\n",
    "\n",
    "# Default Activation Function\n",
    "act = 'sig'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from config import *   #These are parameters in the previous section"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the Network\n",
    "The following section is equivalent to running the following script inside this directory, which you could do by uncommenting and running the cell below - the script would also require there to be two folders of good and bad training data and to give their location. \n",
    "Example: `example: python train.py --good_path F:\\CACO_HOM\\Rectified_imagery\\good --bad_path F:\\CACO_HOM\\Rectified_imagery\\bad`\n",
    "\n",
    "Or - import the functions in the subsequent box and then you can run the main model function - noting that you will have to change the IMG_PATH manually rather than giving it as an argument. To see the meaning of the functions see the script.\n",
    "\n",
    "The script is for pre-processing the data by resizing, median filtering the images before training the Neural Network model for the task of classifying blur and clear images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python train.py --good_path 'path' --bad_path 'path'  #See the .py scrip for all the parameter definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from train import save_data, NN_Model, derivative, cost, back_propagate, get_batch, output_encoding, show_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "print ('Pre-Processsing the Data...........\\n')\n",
    "\n",
    "GOOD_IMG_PATH = 'F:/CACO_HOM/Rectified_imagery/good/'\n",
    "BAD_IMG_PATH = 'F:/CACO_HOM/Rectified_imagery/bad/'\n",
    "    \n",
    "# Reading the Good Images \n",
    "good_img = []\n",
    "for filename in os.listdir(GOOD_IMG_PATH):\n",
    "    good_img.append(ms.imread(GOOD_IMG_PATH+filename, mode='L'))\n",
    "good_img = np.asarray(good_img)\n",
    "\n",
    "# Reading the Bad Images \n",
    "bad_img = []\n",
    "for filename in os.listdir(BAD_IMG_PATH):\n",
    "    bad_img.append(ms.imread(BAD_IMG_PATH+filename, mode='L'))\n",
    "bad_img = np.asarray(bad_img)\n",
    "\n",
    "# Concatenate the array of Good & Bad images\n",
    "combined_img = np.concatenate((good_img, bad_img))  \n",
    "labels = np.concatenate((np.ones(good_img.shape[0]), \n",
    "                        np.zeros(bad_img.shape[0])))\n",
    " \n",
    "# Filtering the combined images to Reduce the Noise present\n",
    "combined_img = nd.median_filter(combined_img, radius)\n",
    "\n",
    "#__________________________________________________________________________________\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Pre-process the data with filtering, resizing \n",
    "    and trained the Neural Networks with \n",
    "    resulting pre-processed data using backpropagation\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    ## Input Layer -> 10001 U\n",
    "    ## 1 Hidden Layers -> 300 HU \n",
    "    ## 1 Output Layer -> 2 Neurons\n",
    "\n",
    "    # Path Validation\n",
    "    if not path_validation(GOOD_IMG_PATH, read_access=True):\n",
    "        exit(0)\n",
    "    if not path_validation(BAD_IMG_PATH, read_access=True):\n",
    "        exit(0)\n",
    "\n",
    "    # Model Path Vaildation\n",
    "    if path_validation(MODEL_PATH):\n",
    "        print ('\\nModel Path Success .....\\n')\n",
    "\n",
    "\n",
    "    # Getting the Same Result in Shuffle in each Run.\n",
    "    np.random.seed(SEED)\n",
    "\n",
    "    # Convert the Good & Bad Images to Cumulative numpy array \n",
    "    #imgs, labels = data_preprocess(GOOD_IMG_PATH, BAD_IMG_PATH, radius=RADIUS)\n",
    "    imgs = combined_img\n",
    "    \n",
    "    # Resizing the feature space for easier to handle\n",
    "    imgs = resize(imgs, width=WIDTH, height=HEIGHT)\n",
    "\n",
    "    # Splitting the Data for Training and Testing Purpose\n",
    "    print('\\nSplitting of Data......\\n')\n",
    "    train_images, test_images, train_labels, test_labels = train_test_split(imgs, labels, \n",
    "                                                        test_size=SPLIT_RATIO, random_state = SEED) \n",
    "\n",
    "    # Saving the splitted data to disk\n",
    "    save_data(train_images, train_labels, test_images, test_labels)\n",
    "\n",
    "    # No of unique class in data\n",
    "    nclass = np.unique(labels).shape[0]\n",
    "    \t\t\t\n",
    "    # Addition of Bias in Train/Test Images\n",
    "    train_images = np.insert(train_images, 0, 1, axis=1) \n",
    "    test_images = np.insert(test_images, 0, 1, axis=1)\n",
    "\n",
    "    # May Used for Cal No Of Neuron as hyper-parameters to Good value\n",
    "    no_of_neurons = train_images.shape[0]/(2*(train_images.shape[1]+10))\n",
    "\n",
    "\n",
    "    # Intializing the Model\n",
    "    theta = NN_Model([train_images.shape[1],NEURONS_SIZE,nclass])\n",
    "\n",
    "    print (\"BAckPROP .................\\n\")\n",
    "    params = back_propagate(theta['Theta1'], theta['Theta2'], train_images, train_labels,\n",
    "                            nclass, alpha=ALPHA, lambdaa=LAMBDA, max_iter=MAX_ITER, act=ACT, \n",
    "                            batch_size=BATCH_SIZE, logging=LOGGING_STEPS)\n",
    "\n",
    "    # Accuracy Score on Train set\n",
    "    accuracy = model_score(params, train_images, train_labels, act=ACT) \n",
    "    print('\\nAccuracy on Train Data: ', accuracy)\n",
    "\n",
    "    # Accuracy Score on test set\n",
    "    accuracy = model_score(params, test_images, test_labels, act=ACT) \n",
    "    print('\\nAccuracy on Test Data: ', accuracy)\n",
    "\n",
    "    # Storing the Results in tmp directory \n",
    "    print ('\\nSaving Results...............\\n')\n",
    "    joblib.dump(params, MODEL_PATH)\n",
    " \n",
    "    # Plotting the Curve\n",
    "    show_plot(params['Loss'], PLOT_PATH)\n",
    "    \n",
    "#________________________________________________________\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n",
    "print ('finished')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the Network\n",
    "Test the neural network with test data stored by train.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Accuracy : 86.60714285714286 %\n",
      "\n"
     ]
    }
   ],
   "source": [
    "params = joblib.load(MODEL_PATH)\n",
    "test_images = np.load(TEST_DATA_PATH)\n",
    "test_labels = np.load(TEST_LABEL_PATH)\n",
    "\n",
    "# Addition of bias in test set\n",
    "test_images = np.insert(test_images, 0, 1, axis=1)\n",
    "\n",
    "accuracy = model_score(params, test_images, test_labels, act='sig')\n",
    "print ('\\nAccuracy : ' + str(accuracy) + ' %\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict Part\n",
    "predict the label of images(Good/Bad) provided by argument while calling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\jover\\anaconda3\\envs\\blurandclear\\lib\\site-packages\\ipykernel_launcher.py:12: DeprecationWarning: `imread` is deprecated!\n",
      "`imread` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``imageio.imread`` instead.\n",
      "  if sys.path[0] == '':\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Resizing the images .........\n",
      "\n",
      "F:\\CACO_HOM\\Rectified_imagery\\predict\\1576267201.jpg is a Good Image\n",
      "\n",
      "\n",
      "Resizing the images .........\n",
      "\n",
      "F:\\CACO_HOM\\Rectified_imagery\\predict\\1576269001.jpg is a Good Image\n",
      "\n",
      "\n",
      "Resizing the images .........\n",
      "\n",
      "F:\\CACO_HOM\\Rectified_imagery\\predict\\1576270801.jpg is a Good Image\n",
      "\n",
      "\n",
      "Resizing the images .........\n",
      "\n",
      "F:\\CACO_HOM\\Rectified_imagery\\predict\\1576512001.jpg is a Bad Image\n",
      "\n",
      "\n",
      "Resizing the images .........\n",
      "\n",
      "F:\\CACO_HOM\\Rectified_imagery\\predict\\1576524601.jpg is a Good Image\n",
      "\n",
      "\n",
      "Resizing the images .........\n",
      "\n",
      "F:\\CACO_HOM\\Rectified_imagery\\predict\\1576526401.jpg is a Good Image\n",
      "\n",
      "\n",
      "Resizing the images .........\n",
      "\n",
      "F:\\CACO_HOM\\Rectified_imagery\\predict\\1576528201.jpg is a Good Image\n",
      "\n",
      "\n",
      "Resizing the images .........\n",
      "\n",
      "F:\\CACO_HOM\\Rectified_imagery\\predict\\1576530001.jpg is a Bad Image\n",
      "\n",
      "\n",
      "Resizing the images .........\n",
      "\n",
      "F:\\CACO_HOM\\Rectified_imagery\\predict\\1576584001.jpg is a Bad Image\n",
      "\n",
      "\n",
      "Resizing the images .........\n",
      "\n",
      "F:\\CACO_HOM\\Rectified_imagery\\predict\\1576596601.jpg is a Good Image\n",
      "\n",
      "\n",
      "Resizing the images .........\n",
      "\n",
      "F:\\CACO_HOM\\Rectified_imagery\\predict\\1576602001.jpg is a Bad Image\n",
      "\n",
      "\n",
      "Resizing the images .........\n",
      "\n",
      "F:\\CACO_HOM\\Rectified_imagery\\predict\\1576603801.jpg is a Bad Image\n",
      "\n",
      "\n",
      "Resizing the images .........\n",
      "\n",
      "F:\\CACO_HOM\\Rectified_imagery\\predict\\1576681201.jpg is a Bad Image\n",
      "\n",
      "\n",
      "Resizing the images .........\n",
      "\n",
      "F:\\CACO_HOM\\Rectified_imagery\\predict\\1576683001.jpg is a Bad Image\n",
      "\n",
      "\n",
      "Resizing the images .........\n",
      "\n",
      "F:\\CACO_HOM\\Rectified_imagery\\predict\\1576684801.jpg is a Bad Image\n",
      "\n",
      "\n",
      "Resizing the images .........\n",
      "\n",
      "F:\\CACO_HOM\\Rectified_imagery\\predict\\1576686601.jpg is a Bad Image\n",
      "\n",
      "\n",
      "Resizing the images .........\n",
      "\n",
      "F:\\CACO_HOM\\Rectified_imagery\\predict\\1576688401.jpg is a Bad Image\n",
      "\n",
      "\n",
      "Resizing the images .........\n",
      "\n",
      "F:\\CACO_HOM\\Rectified_imagery\\predict\\1576690201.jpg is a Bad Image\n",
      "\n",
      "\n",
      "Resizing the images .........\n",
      "\n",
      "F:\\CACO_HOM\\Rectified_imagery\\predict\\1576692001.jpg is a Good Image\n",
      "\n",
      "\n",
      "Resizing the images .........\n",
      "\n",
      "F:\\CACO_HOM\\Rectified_imagery\\predict\\1576693801.jpg is a Good Image\n",
      "\n",
      "\n",
      "Resizing the images .........\n",
      "\n",
      "F:\\CACO_HOM\\Rectified_imagery\\predict\\1576695601.jpg is a Bad Image\n",
      "\n",
      "\n",
      "Resizing the images .........\n",
      "\n",
      "F:\\CACO_HOM\\Rectified_imagery\\predict\\1576697401.jpg is a Good Image\n",
      "\n",
      "\n",
      "Resizing the images .........\n",
      "\n",
      "F:\\CACO_HOM\\Rectified_imagery\\predict\\1576699201.jpg is a Good Image\n",
      "\n",
      "\n",
      "Resizing the images .........\n",
      "\n",
      "F:\\CACO_HOM\\Rectified_imagery\\predict\\1576701001.jpg is a Good Image\n",
      "\n",
      "\n",
      "Resizing the images .........\n",
      "\n",
      "F:\\CACO_HOM\\Rectified_imagery\\predict\\1576702801.jpg is a Good Image\n",
      "\n",
      "\n",
      "Resizing the images .........\n",
      "\n",
      "F:\\CACO_HOM\\Rectified_imagery\\predict\\1576756801.jpg is a Good Image\n",
      "\n",
      "\n",
      "Resizing the images .........\n",
      "\n",
      "F:\\CACO_HOM\\Rectified_imagery\\predict\\1576758601.jpg is a Good Image\n",
      "\n",
      "\n",
      "Resizing the images .........\n",
      "\n",
      "F:\\CACO_HOM\\Rectified_imagery\\predict\\1576760401.jpg is a Bad Image\n",
      "\n",
      "\n",
      "Resizing the images .........\n",
      "\n",
      "F:\\CACO_HOM\\Rectified_imagery\\predict\\1576762201.jpg is a Bad Image\n",
      "\n",
      "\n",
      "Resizing the images .........\n",
      "\n",
      "F:\\CACO_HOM\\Rectified_imagery\\predict\\1576764001.jpg is a Bad Image\n",
      "\n",
      "\n",
      "Resizing the images .........\n",
      "\n",
      "F:\\CACO_HOM\\Rectified_imagery\\predict\\1576765801.jpg is a Bad Image\n",
      "\n",
      "\n",
      "Resizing the images .........\n",
      "\n",
      "F:\\CACO_HOM\\Rectified_imagery\\predict\\1576767601.jpg is a Bad Image\n",
      "\n",
      "\n",
      "Resizing the images .........\n",
      "\n",
      "F:\\CACO_HOM\\Rectified_imagery\\predict\\1576769401.jpg is a Good Image\n",
      "\n",
      "\n",
      "Resizing the images .........\n",
      "\n",
      "F:\\CACO_HOM\\Rectified_imagery\\predict\\1576771201.jpg is a Bad Image\n",
      "\n",
      "\n",
      "Resizing the images .........\n",
      "\n",
      "F:\\CACO_HOM\\Rectified_imagery\\predict\\1576773001.jpg is a Bad Image\n",
      "\n",
      "\n",
      "Resizing the images .........\n",
      "\n",
      "F:\\CACO_HOM\\Rectified_imagery\\predict\\1576774801.jpg is a Bad Image\n",
      "\n",
      "\n",
      "Resizing the images .........\n",
      "\n",
      "F:\\CACO_HOM\\Rectified_imagery\\predict\\1576776601.jpg is a Bad Image\n",
      "\n",
      "\n",
      "Resizing the images .........\n",
      "\n",
      "F:\\CACO_HOM\\Rectified_imagery\\predict\\1576778401.jpg is a Good Image\n",
      "\n",
      "\n",
      "Resizing the images .........\n",
      "\n",
      "F:\\CACO_HOM\\Rectified_imagery\\predict\\1576780201.jpg is a Good Image\n",
      "\n",
      "\n",
      "Resizing the images .........\n",
      "\n",
      "F:\\CACO_HOM\\Rectified_imagery\\predict\\1576785601.jpg is a Bad Image\n",
      "\n",
      "\n",
      "Resizing the images .........\n",
      "\n",
      "F:\\CACO_HOM\\Rectified_imagery\\predict\\1576789201.jpg is a Good Image\n",
      "\n",
      "\n",
      "Resizing the images .........\n",
      "\n",
      "F:\\CACO_HOM\\Rectified_imagery\\predict\\1576843201.jpg is a Good Image\n",
      "\n",
      "\n",
      "Resizing the images .........\n",
      "\n",
      "F:\\CACO_HOM\\Rectified_imagery\\predict\\1576845001.jpg is a Bad Image\n",
      "\n",
      "\n",
      "Resizing the images .........\n",
      "\n",
      "F:\\CACO_HOM\\Rectified_imagery\\predict\\1576846801.jpg is a Bad Image\n",
      "\n",
      "\n",
      "Resizing the images .........\n",
      "\n",
      "F:\\CACO_HOM\\Rectified_imagery\\predict\\1576848601.jpg is a Bad Image\n",
      "\n",
      "\n",
      "Resizing the images .........\n",
      "\n",
      "F:\\CACO_HOM\\Rectified_imagery\\predict\\1576850401.jpg is a Bad Image\n",
      "\n",
      "\n",
      "Resizing the images .........\n",
      "\n",
      "F:\\CACO_HOM\\Rectified_imagery\\predict\\1576852201.jpg is a Bad Image\n",
      "\n",
      "\n",
      "Resizing the images .........\n",
      "\n",
      "F:\\CACO_HOM\\Rectified_imagery\\predict\\1576854001.jpg is a Bad Image\n",
      "\n",
      "\n",
      "Resizing the images .........\n",
      "\n",
      "F:\\CACO_HOM\\Rectified_imagery\\predict\\1576855801.jpg is a Bad Image\n",
      "\n",
      "\n",
      "Resizing the images .........\n",
      "\n",
      "F:\\CACO_HOM\\Rectified_imagery\\predict\\1576857601.jpg is a Bad Image\n",
      "\n",
      "\n",
      "Resizing the images .........\n",
      "\n",
      "F:\\CACO_HOM\\Rectified_imagery\\predict\\1576859401.jpg is a Bad Image\n",
      "\n",
      "\n",
      "Resizing the images .........\n",
      "\n",
      "F:\\CACO_HOM\\Rectified_imagery\\predict\\1576861201.jpg is a Bad Image\n",
      "\n",
      "\n",
      "Resizing the images .........\n",
      "\n",
      "F:\\CACO_HOM\\Rectified_imagery\\predict\\1576863001.jpg is a Bad Image\n",
      "\n",
      "\n",
      "Resizing the images .........\n",
      "\n",
      "F:\\CACO_HOM\\Rectified_imagery\\predict\\1576864801.jpg is a Bad Image\n",
      "\n",
      "\n",
      "Resizing the images .........\n",
      "\n",
      "F:\\CACO_HOM\\Rectified_imagery\\predict\\1576868401.jpg is a Bad Image\n",
      "\n",
      "\n",
      "Resizing the images .........\n",
      "\n",
      "F:\\CACO_HOM\\Rectified_imagery\\predict\\1576870201.jpg is a Bad Image\n",
      "\n",
      "\n",
      "Resizing the images .........\n",
      "\n",
      "F:\\CACO_HOM\\Rectified_imagery\\predict\\1576875601.jpg is a Good Image\n",
      "\n",
      "Wall time: 1.18 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "#output image locations\n",
    "bad_p = 'F:\\\\CACO_HOM\\\\predicted_bad\\\\'\n",
    "good_p = 'F:\\\\CACO_HOM\\\\predicted_good\\\\'\n",
    "    \n",
    "#images to predict\n",
    "predict_path = 'F:\\\\CACO_HOM\\\\Rectified_imagery\\\\predict\\\\'\n",
    "filenames = sorted(glob.glob(predict_path + '1576*.jpg'))\n",
    "#files = sorted(os.listdir(predict_path))\n",
    "\n",
    "for i in filenames:\n",
    "    # Reading images in grayscale mode\n",
    "    img = ms.imread(i, mode='L')\n",
    "    # Applying median filter to remove noise\n",
    "    img = nd.median_filter(img, radius)\n",
    "    # To make it 2D\n",
    "    img = img[np.newaxis, :]\n",
    "    # Resizing the images to that of train\n",
    "    img = resize(img, width=width, height=height)\n",
    "    # Addition of bias term\n",
    "    img = np.insert(img, 0, 1, axis=1)\n",
    "    \n",
    "    # preprocess the images\n",
    "    #img = predict_preprocess(i)\n",
    "    \n",
    "    ## Find the label predicted by the model\n",
    "    predicted_label = validate(params, img)\n",
    "    \n",
    "    files = sorted(os.listdir(predict_path))\n",
    "    #for label in predicted_label:\n",
    "        #for f in files:\n",
    "            #if label:\n",
    "                #print(f + \" is a Good Image\\n\")\n",
    "                #shutil.move(predict_path+f, good_p + f)\n",
    "            #else:\n",
    "                #print(f+ \" is a Bad Image\\n\")\n",
    "            #shutil.move(predict_path+f, bad_p + f)\n",
    "    \n",
    "    for label in predicted_label:\n",
    "        if label:\n",
    "            print(i + \" is a Good Image\\n\")\n",
    "            shutil.copy(i, good_p + i[38:52])\n",
    "        else:\n",
    "            print(i + \" is a Bad Image\\n\")\n",
    "        shutil.copy(i, bad_p + i[38:52])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "print(filenames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
